{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oTvimmyCW3m"
      },
      "outputs": [],
      "source": [
        "# 1. –û–±–Ω–æ–≤–ª—è–µ–º pip –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å–±–æ—Ä–∫–∏ (—ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏)\n",
        "!pip install --upgrade pip setuptools wheel cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8tzYUWSCw0u"
      },
      "outputs": [],
      "source": [
        "!pip install Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bMKNUGu2CzNf"
      },
      "outputs": [],
      "source": [
        "# 2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏ (—Ç–∞–∫ –Ω–∞–¥–µ–∂–Ω–µ–µ)\n",
        "!pip install catboost\n",
        "!pip install pandas numpy scikit-learn\n",
        "\n",
        "# 3. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º implicit —Å —Ñ–ª–∞–≥–æ–º verbose, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –ø—Ä–æ—Ü–µ—Å—Å (–µ—Å–ª–∏ –±—É–¥–µ—Ç –∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–æ–ª–≥–æ)\n",
        "# –û–±—ã—á–Ω–æ –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cmake –¥–µ–ª–æ –∏–¥–µ—Ç –±—ã—Å—Ç—Ä–µ–µ\n",
        "!pip install implicit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7IwLLguC7_A"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sparse\n",
        "import implicit\n",
        "from catboost import CatBoostRanker, Pool\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# --- MASTER CONFIGURATION (STRICT MODE) ---\n",
        "class Config:\n",
        "    RANDOM_STATE = 42\n",
        "    VAL_SIZE = 0.1\n",
        "\n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ALS\n",
        "    ALS_FACTORS = 64\n",
        "    ALS_REGULARIZATION = 0.05\n",
        "    ALS_ITERATIONS = 15\n",
        "    N_FOLDS_ALS = 5           # 5 —Ñ–æ–ª–¥–æ–≤ - –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç\n",
        "\n",
        "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
        "    TOP_K_CANDIDATES = 40     # Hard Negatives –æ—Ç ALS\n",
        "    NUM_RANDOM_NEGS = 0     # Random Negatives (–¥–ª—è –±–∞–ª–∞–Ω—Å–∞)\n",
        "\n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ NLP\n",
        "    USE_BERT = True         # –í–∫–ª—é—á–µ–Ω–æ\n",
        "    BERT_MODEL = 'cointegrated/rubert-tiny2'\n",
        "    BERT_PCA_COMPONENTS = 32\n",
        "\n",
        "\n",
        "    USE_TFIDF=True\n",
        "    TFIDF_MAX_FEATURES=5000\n",
        "    TFIDF_SVD_COMPONENTS=24\n",
        "\n",
        "\n",
        "\n",
        "    # –§–∏—á–∏\n",
        "    USE_META_FEATURES = True\n",
        "    USE_LOO_STATS = True      # –û—Å—Ç–∞–≤–ª—è–µ–º Leave-One-Out (–±–µ–∑–æ–ø–∞—Å–Ω–æ)\n",
        "    USE_TIME_FEATURES = True\n",
        "\n",
        "    CAT_COLS = ['author_id', 'publisher', 'language', 'category_id']\n",
        "\n",
        "CFG = Config()\n",
        "print(\"Strict Config loaded.\")\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ (–û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è)\n",
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type != object and not str(col_type).startswith('datetime'):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "    print(f'Memory usage reduced to {df.memory_usage().sum() / 1024**2:.2f} MB')\n",
        "    return df\n",
        "\n",
        "print(\"Config loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr0fdU_8DI0s"
      },
      "outputs": [],
      "source": [
        "print(\"Loading data...\")\n",
        "# –ó–∞–º–µ–Ω–∏—Ç–µ –ø—É—Ç–∏ –Ω–∞ —Å–≤–æ–∏\n",
        "train_df = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
        "candidates_df = pd.read_csv('candidates.csv')\n",
        "users_df = pd.read_csv('users.csv')\n",
        "books_df = pd.read_csv('books.csv')\n",
        "book_descriptions = pd.read_csv(\"book_descriptions.csv\")\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
        "train_df = reduce_mem_usage(train_df)\n",
        "users_df = reduce_mem_usage(users_df)\n",
        "books_df = reduce_mem_usage(books_df)\n",
        "\n",
        "# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ ID (–≤–∞–∂–Ω–æ –¥–ª—è ALS)\n",
        "unique_users = train_df['user_id'].unique()\n",
        "unique_items = books_df['book_id'].unique()\n",
        "\n",
        "user2idx = {u: i for i, u in enumerate(unique_users)}\n",
        "item2idx = {i: k for k, i in enumerate(unique_items)}\n",
        "idx2user = {i: u for u, i in user2idx.items()}\n",
        "idx2item = {k: i for i, k in item2idx.items()}\n",
        "\n",
        "num_users = len(unique_users)\n",
        "num_items = len(unique_items)\n",
        "\n",
        "print(f\"Users: {num_users}, Items: {num_items}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA-H7RjcDPuH"
      },
      "outputs": [],
      "source": [
        "def process_text_features(desc_df):\n",
        "    print(\"--- Processing Text Features ---\")\n",
        "    desc_df['description'] = desc_df['description'].fillna('')\n",
        "    final_emb_df = pd.DataFrame({'book_id': desc_df['book_id']})\n",
        "\n",
        "    # 1. TF-IDF + SVD\n",
        "    if CFG.USE_TFIDF:\n",
        "        print(\"Running TF-IDF + SVD...\")\n",
        "        tfidf = TfidfVectorizer(max_features=CFG.TFIDF_MAX_FEATURES, stop_words='english')\n",
        "        tfidf_mat = tfidf.fit_transform(desc_df['description'])\n",
        "\n",
        "        svd = TruncatedSVD(n_components=CFG.TFIDF_SVD_COMPONENTS, random_state=CFG.RANDOM_STATE)\n",
        "        svd_mat = svd.fit_transform(tfidf_mat)\n",
        "\n",
        "        cols = [f'tfidf_svd_{i}' for i in range(CFG.TFIDF_SVD_COMPONENTS)]\n",
        "        temp_df = pd.DataFrame(svd_mat, columns=cols)\n",
        "        final_emb_df = pd.concat([final_emb_df, temp_df], axis=1)\n",
        "\n",
        "    # 2. BERT + PCA\n",
        "    if CFG.USE_BERT:\n",
        "        print(f\"Running BERT ({CFG.BERT_MODEL})...\")\n",
        "        model_bert = SentenceTransformer(CFG.BERT_MODEL)\n",
        "        # –ï—Å–ª–∏ –µ—Å—Ç—å GPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model_bert.to(device)\n",
        "\n",
        "        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è, –Ω–æ rubert-tiny –±—ã—Å—Ç—Ä—ã–π)\n",
        "        embeddings = model_bert.encode(\n",
        "            desc_df['description'].tolist(),\n",
        "            show_progress_bar=True,\n",
        "            batch_size=64,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # –°–∂–∏–º–∞–µ–º PCA\n",
        "        print(\"Reducing BERT dimensions with PCA...\")\n",
        "        pca = PCA(n_components=CFG.BERT_PCA_COMPONENTS, random_state=CFG.RANDOM_STATE)\n",
        "        bert_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "        cols = [f'bert_pca_{i}' for i in range(CFG.BERT_PCA_COMPONENTS)]\n",
        "        temp_df = pd.DataFrame(bert_pca, columns=cols)\n",
        "        final_emb_df = pd.concat([final_emb_df, temp_df], axis=1)\n",
        "\n",
        "    return reduce_mem_usage(final_emb_df)\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
        "if book_descriptions is not None:\n",
        "    text_features_df = process_text_features(book_descriptions)\n",
        "else:\n",
        "    text_features_df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-U3A7MDDUb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "import implicit\n",
        "from sklearn.model_selection import KFold\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# –ú–∞–ø–ø–∏–Ω–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã —Ä–∞–Ω–µ–µ, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –æ–±–Ω–æ–≤–∏–º –∏—Ö –≥–ª–æ–±–∞–ª—å–Ω–æ\n",
        "unique_users = train_df['user_id'].unique()\n",
        "unique_items = books_df['book_id'].unique()\n",
        "user2idx = {u: i for i, u in enumerate(unique_users)}\n",
        "item2idx = {i: k for k, i in enumerate(unique_items)}\n",
        "idx2user = {i: u for u, i in user2idx.items()}\n",
        "idx2item = {k: i for i, k in item2idx.items()}\n",
        "num_users, num_items = len(unique_users), len(unique_items)\n",
        "\n",
        "def generate_hybrid_train_data_v8(df, n_hard=40, n_random=10, n_folds=5):\n",
        "    print(f\"--- [v8] Generating Hybrid Train Data (Global Candidates + OOF Scores) ---\")\n",
        "\n",
        "    # === –≠–¢–ê–ü 1: –û–±—É—á–µ–Ω–∏–µ –ì–ª–æ–±–∞–ª—å–Ω–æ–π ALS (–¥–ª—è –≤—ã–±–æ—Ä–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤) ===\n",
        "    print(\"1. Training Global ALS (Candidate Selection)...\")\n",
        "    rows = df['user_id'].map(user2idx).values\n",
        "    cols = df['book_id'].map(item2idx).values\n",
        "    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
        "    sparse_global = sparse.csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(num_users, num_items))\n",
        "\n",
        "    global_model = implicit.als.AlternatingLeastSquares(\n",
        "        factors=CFG.ALS_FACTORS,\n",
        "        regularization=CFG.ALS_REGULARIZATION,\n",
        "        iterations=CFG.ALS_ITERATIONS,\n",
        "        random_state=CFG.RANDOM_STATE,\n",
        "        use_gpu=torch.cuda.is_available()\n",
        "    )\n",
        "    global_model.fit(sparse_global, show_progress=True)\n",
        "\n",
        "    # === –≠–¢–ê–ü 2: –°–±–æ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ (–°—Ç—Ä–æ–∫–∏) ===\n",
        "    print(f\"2. Constructing Dataset Skeleton (Pos + {n_hard} Hard + {n_random} Random)...\")\n",
        "\n",
        "    # –ê) –ü–æ–∑–∏—Ç–∏–≤—ã (Target: 2 = Read, 1 = Planned)\n",
        "    pos_df = df[['user_id', 'book_id', 'has_read']].copy()\n",
        "    pos_df['target'] = pos_df['has_read'].apply(lambda x: 2.0 if x == 1 else 1.0)\n",
        "    pos_df.drop(columns=['has_read'], inplace=True)\n",
        "\n",
        "    # –ë) Hard Negatives (Target: 0) - –±–µ—Ä–µ–º —Ç–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "    # –≠—Ç–æ —Å–∞–º—ã–µ \"—Å–ª–æ–∂–Ω—ã–µ\" –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ –º–æ–¥–µ–ª—å –æ—à–∏–±–∞–µ—Ç—Å—è\n",
        "    all_users_idx = np.arange(num_users)\n",
        "    ids, _ = global_model.recommend(\n",
        "        all_users_idx,\n",
        "        sparse_global[all_users_idx],\n",
        "        N=n_hard,\n",
        "        filter_already_liked_items=True\n",
        "    )\n",
        "\n",
        "    neg_data = []\n",
        "    # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "    for i, u_idx in enumerate(all_users_idx):\n",
        "        u_id = idx2user[u_idx]\n",
        "        for b_idx in ids[i]:\n",
        "            # Target 0\n",
        "            neg_data.append([u_id, idx2item[b_idx], 0.0])\n",
        "\n",
        "    # –í) Random Negatives (Target: 0) - –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏\n",
        "    rnd_data = []\n",
        "    if n_random > 0:\n",
        "        for u_idx in all_users_idx:\n",
        "            u_id = idx2user[u_idx]\n",
        "            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∫–Ω–∏–≥–∏ (–º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –Ω–æ —Ü–∏–∫–ª –Ω–∞–¥–µ–∂–Ω–µ–µ)\n",
        "            rand_items_idx = np.random.randint(0, num_items, n_random)\n",
        "            for r_idx in rand_items_idx:\n",
        "                rnd_data.append([u_id, idx2item[r_idx], 0.0])\n",
        "\n",
        "    # –°–±–æ—Ä–∫–∞\n",
        "    neg_df = pd.DataFrame(neg_data, columns=['user_id', 'book_id', 'target'])\n",
        "    rnd_df = pd.DataFrame(rnd_data, columns=['user_id', 'book_id', 'target'])\n",
        "\n",
        "    full_df = pd.concat([pos_df, neg_df, rnd_df], ignore_index=True)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Pos > Hard > Random)\n",
        "    full_df.sort_values(by='target', ascending=False, inplace=True)\n",
        "    full_df.drop_duplicates(subset=['user_id', 'book_id'], keep='first', inplace=True)\n",
        "\n",
        "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è OOF —Å–∫–æ—Ä–æ–≤ (–ø–æ–∫–∞ –ø—É—Å—Ç—ã–µ)\n",
        "    full_df['als_score'] = np.nan\n",
        "    full_df['als_item_norm'] = np.nan\n",
        "\n",
        "    # –ü–æ–º–µ—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ç–∏–≤—ã, —á—Ç–æ–±—ã –∑–Ω–∞—Ç—å, —á—Ç–æ –∏—Å–∫–ª—é—á–∞—Ç—å –ø—Ä–∏ OOF –æ–±—É—á–µ–Ω–∏–∏\n",
        "    # –°–æ–∑–¥–∞–µ–º set –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: (u_idx, b_idx)\n",
        "    real_interactions = set(zip(\n",
        "        pos_df['user_id'].map(user2idx).values,\n",
        "        pos_df['book_id'].map(item2idx).values\n",
        "    ))\n",
        "\n",
        "    print(f\"Skeleton created. Rows: {len(full_df)}. Starting OOF Scoring...\")\n",
        "\n",
        "    # === –≠–¢–ê–ü 3: OOF Scoring (–ß–µ—Å—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ–≤) ===\n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º –í–ï–°–¨ full_df –Ω–∞ —Ñ–æ–ª–¥—ã. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞ –æ–±—É—á–∞–µ–º ALS –∑–∞–Ω–æ–≤–æ,\n",
        "    # –ò–°–ö–õ–Æ–ß–ê–Ø –∏–∑ –æ–±—É—á–µ–Ω–∏—è —Ç–µ –ø–æ–∑–∏—Ç–∏–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ø–∞–ª–∏ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏—é.\n",
        "\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
        "\n",
        "    # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã (–≤—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è)\n",
        "    train_u_indices = df['user_id'].map(user2idx).values\n",
        "    train_b_indices = df['book_id'].map(item2idx).values\n",
        "\n",
        "    fold_cnt = 0\n",
        "    for _, val_idx in kf.split(full_df):\n",
        "        fold_cnt += 1\n",
        "\n",
        "        # –°—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Å–µ–π—á–∞—Å –±—É–¥–µ–º —Å–∫–æ—Ä–∏—Ç—å\n",
        "        current_rows = full_df.iloc[val_idx]\n",
        "\n",
        "        # –ù–∞—Ö–æ–¥–∏–º —Å—Ä–µ–¥–∏ –Ω–∏—Ö —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –†–ï–ê–õ–¨–ù–´–ú–ò –ø–æ–∑–∏—Ç–∏–≤–∞–º–∏\n",
        "        # –ò—Ö –Ω—É–∂–Ω–æ \"—Å–ø—Ä—è—Ç–∞—Ç—å\" –æ—Ç ALS\n",
        "        current_u_idxs = current_rows['user_id'].map(user2idx).fillna(-1).astype(int).values\n",
        "        current_b_idxs = current_rows['book_id'].map(item2idx).fillna(-1).astype(int).values\n",
        "\n",
        "        # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä—ã (u, i), –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–¥–æ –∏—Å–∫–ª—é—á–∏—Ç—å –∏–∑ –æ–±—É—á–µ–Ω–∏—è\n",
        "        pairs_to_exclude = set()\n",
        "        for u, b in zip(current_u_idxs, current_b_idxs):\n",
        "            if (u, b) in real_interactions:\n",
        "                pairs_to_exclude.add((u, b))\n",
        "\n",
        "        # –°—Ç—Ä–æ–∏–º –æ–±—É—á–∞—é—â—É—é –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–æ–ª–¥–∞\n",
        "        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π train_df\n",
        "        mask_keep = []\n",
        "        for u, b in zip(train_u_indices, train_b_indices):\n",
        "            if (u, b) in pairs_to_exclude:\n",
        "                mask_keep.append(False)\n",
        "            else:\n",
        "                mask_keep.append(True)\n",
        "\n",
        "        mask_keep = np.array(mask_keep)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É (–±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ç–∏–≤–æ–≤)\n",
        "        sparse_fold = sparse.csr_matrix(\n",
        "            (np.ones(mask_keep.sum()), (train_u_indices[mask_keep], train_b_indices[mask_keep])),\n",
        "            shape=(num_users, num_items)\n",
        "        )\n",
        "\n",
        "        # –û–±—É—á–∞–µ–º ALS –¥–ª—è —Ñ–æ–ª–¥–∞\n",
        "        model_fold = implicit.als.AlternatingLeastSquares(\n",
        "            factors=CFG.ALS_FACTORS, regularization=CFG.ALS_REGULARIZATION,\n",
        "            iterations=CFG.ALS_ITERATIONS, random_state=CFG.RANDOM_STATE,\n",
        "            use_gpu=torch.cuda.is_available()\n",
        "        )\n",
        "        model_fold.fit(sparse_fold, show_progress=False)\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º (Dot Product)\n",
        "        if hasattr(model_fold.user_factors, \"to_numpy\"):\n",
        "            u_fact = model_fold.user_factors.to_numpy()\n",
        "            i_fact = model_fold.item_factors.to_numpy()\n",
        "        else:\n",
        "            u_fact = model_fold.user_factors\n",
        "            i_fact = model_fold.item_factors\n",
        "\n",
        "        # –ë–µ—Ä–µ–º –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ–∫—É—â–∏—Ö —é–∑–µ—Ä–æ–≤ –∏ –∫–Ω–∏–≥ (–¥–∞–∂–µ –µ—Å–ª–∏ —é–∑–µ—Ä –±—ã–ª —É–¥–∞–ª–µ–Ω –∏–∑ —Ç—Ä–µ–π–Ω–∞, –≤–µ–∫—Ç–æ—Ä –±—É–¥–µ—Ç, —Ö–æ—Ç—å –∏ –ø–ª–æ—Ö–æ–π)\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ -1 –Ω–µ –Ω—É–∂–Ω–∞, —Ç–∞–∫ –∫–∞–∫ skeleton –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID\n",
        "        vectors_u = u_fact[current_u_idxs]\n",
        "        vectors_b = i_fact[current_b_idxs]\n",
        "\n",
        "        scores = (vectors_u * vectors_b).sum(axis=1)\n",
        "        norms = np.linalg.norm(vectors_b, axis=1)\n",
        "\n",
        "        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º\n",
        "        full_df.iloc[val_idx, full_df.columns.get_loc('als_score')] = scores\n",
        "        full_df.iloc[val_idx, full_df.columns.get_loc('als_item_norm')] = norms\n",
        "\n",
        "        print(f\"Fold {fold_cnt}/{n_folds} scored.\")\n",
        "\n",
        "        # –ß–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å\n",
        "        del model_fold, sparse_fold, vectors_u, vectors_b\n",
        "        gc.collect()\n",
        "\n",
        "    full_df = full_df.sample(frac=1, random_state=CFG.RANDOM_STATE).reset_index(drop=True)\n",
        "    print(f\"Hybrid Dataset v8 Ready.\\nTarget Counts:\\n{full_df['target'].value_counts()}\")\n",
        "\n",
        "    return reduce_mem_usage(full_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXWVKxMDDcBU"
      },
      "outputs": [],
      "source": [
        "# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ì–õ–û–ë–ê–õ–¨–ù–´–• —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ (–æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –≤–µ—Å—å –Ω–æ—É—Ç–±—É–∫)\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º \"–¥–æ—á–∏—Ç—ã–≤–∞–µ–º–æ—Å—Ç—å\" (read_ratio) - —Å–∏–ª—å–Ω–∞—è —Ñ–∏—á–∞ –∏–∑ v4\n",
        "global_book_stats = train_df.groupby('book_id').agg({\n",
        "    'rating': ['mean', 'count'],\n",
        "    'has_read': ['mean', 'sum']\n",
        "})\n",
        "global_book_stats.columns = ['book_mean', 'book_count', 'book_read_ratio', 'book_read_sum']\n",
        "\n",
        "global_user_stats = train_df.groupby('user_id').agg({\n",
        "    'rating': ['mean', 'count'],\n",
        "    'has_read': ['mean', 'sum']\n",
        "})\n",
        "global_user_stats.columns = ['user_mean', 'user_count', 'user_read_ratio', 'user_read_sum']\n",
        "\n",
        "# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "GLOBAL_MEAN_RATING = train_df['rating'].mean()\n",
        "GLOBAL_MEAN_READ = train_df['has_read'].mean()\n",
        "\n",
        "def feature_engineering_v8(df):\n",
        "    print(f\"--- Feature Engineering v8 (Consistent/No-LOO) ---\")\n",
        "    out_df = df.copy()\n",
        "\n",
        "    # 1. Meta Features\n",
        "    if CFG.USE_META_FEATURES:\n",
        "        out_df = out_df.merge(users_df, on='user_id', how='left')\n",
        "        out_df = out_df.merge(books_df, on='book_id', how='left')\n",
        "        for c in CFG.CAT_COLS:\n",
        "            if c in out_df.columns:\n",
        "                out_df[c] = out_df[c].fillna(-1).astype(int)\n",
        "\n",
        "    # 2. Text Features (BERT/TF-IDF)\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π text_features_df\n",
        "    if 'text_features_df' in globals() and text_features_df is not None:\n",
        "        out_df = out_df.merge(text_features_df, on='book_id', how='left')\n",
        "        feat_cols = [c for c in text_features_df.columns if c != 'book_id']\n",
        "        out_df[feat_cols] = out_df[feat_cols].fillna(0.0)\n",
        "\n",
        "    # 3. Statistics (Global Merge)\n",
        "    # –ü—Ä–æ—Å—Ç–æ –º–µ—Ä–∂–∏–º. –ù–ï –¥–µ–ª–∞–µ–º –≤—ã—á–∏—Ç–∞–Ω–∏–µ (LOO).\n",
        "    # –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–π –ª–∏–∫ –Ω–∞ —Ç—Ä–µ–π–Ω–µ, –Ω–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ç–µ—Å—Ç–æ–º.\n",
        "    out_df = out_df.merge(global_book_stats, on='book_id', how='left')\n",
        "    out_df = out_df.merge(global_user_stats, on='user_id', how='left')\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "    out_df['book_mean'] = out_df['book_mean'].fillna(GLOBAL_MEAN_RATING)\n",
        "    out_df['book_count'] = out_df['book_count'].fillna(0)\n",
        "    out_df['book_read_ratio'] = out_df['book_read_ratio'].fillna(GLOBAL_MEAN_READ)\n",
        "    out_df['book_read_sum'] = out_df['book_read_sum'].fillna(0)\n",
        "\n",
        "    out_df['user_mean'] = out_df['user_mean'].fillna(GLOBAL_MEAN_RATING)\n",
        "    out_df['user_count'] = out_df['user_count'].fillna(0)\n",
        "    out_df['user_read_ratio'] = out_df['user_read_ratio'].fillna(GLOBAL_MEAN_READ)\n",
        "    out_df['user_read_sum'] = out_df['user_read_sum'].fillna(0)\n",
        "\n",
        "    # 4. Time Features\n",
        "    if CFG.USE_TIME_FEATURES:\n",
        "        cur_year = 2025\n",
        "        if 'publication_year' in out_df.columns:\n",
        "            out_df['publication_year'] = out_df['publication_year'].fillna(cur_year)\n",
        "            out_df['book_age'] = cur_year - out_df['publication_year']\n",
        "            # –ö–ª–∏–ø–∞–µ–º –≤—ã–±—Ä–æ—Å—ã\n",
        "            out_df['book_age'] = out_df['book_age'].clip(0, 200)\n",
        "\n",
        "    # –ß–∏—Å—Ç–∫–∞ –ª–∏—à–Ω–µ–≥–æ\n",
        "    drops = ['title', 'author_name', 'description', 'timestamp', 'has_read']\n",
        "    out_df.drop([c for c in drops if c in out_df.columns], axis=1, inplace=True)\n",
        "\n",
        "    return reduce_mem_usage(out_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oKY4IZ9_QVP"
      },
      "outputs": [],
      "source": [
        "# 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º \"—á–µ—Å—Ç–Ω—ã–π\" –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç (–≤–∫–ª—é—á–∞—è –ø–æ–∑–∏—Ç–∏–≤—ã –∏ –Ω–µ–≥–∞—Ç–∏–≤—ã)\n",
        "train_dataset = generate_hybrid_train_data_v8(train_df, n_hard=CFG.TOP_K_CANDIDATES, n_random=CFG.NUM_RANDOM_NEGS, n_folds=CFG.N_FOLDS_ALS)\n",
        "# 2. –ù–∞–≤–µ—à–∏–≤–∞–µ–º —Ñ–∏—á–∏ (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å LOO)\n",
        "# mode='train' –≤–∫–ª—é—á–∏—Ç –∑–∞—â–∏—Ç—É –æ—Ç –ª–∏–∫–∞ —Ç–∞—Ä–≥–µ—Ç–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞—Ö\n",
        "X_train_strict = feature_engineering_v8(train_dataset)\n",
        "\n",
        "print(\"\\nReady for CatBoost Training.\")\n",
        "print(\"Train Shape:\", X_train_strict.shape)\n",
        "print(\"Columns:\", list(X_train_strict.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtR1v854DeNf"
      },
      "outputs": [],
      "source": [
        "print(\"--- Starting Feature Importance Audit ---\")\n",
        "X_train_full=X_train_strict\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞—É–¥–∏—Ç–∞\n",
        "# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –¥–ª—è YetiRank\n",
        "X_train_full.sort_values(by='user_id', inplace=True)\n",
        "\n",
        "audit_cols_drop = ['user_id', 'book_id', 'target']\n",
        "X_audit = X_train_full.drop(audit_cols_drop, axis=1)\n",
        "y_audit = X_train_full['target']\n",
        "group_audit = X_train_full['user_id']\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –æ—Å—Ç–∞–ª–∏—Å—å\n",
        "audit_cat = [c for c in CFG.CAT_COLS if c in X_audit.columns]\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—ã–π Ranker\n",
        "audit_model = CatBoostRanker(\n",
        "    loss_function='YetiRank',\n",
        "    iterations=700,        # –ú–∞–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    task_type='GPU',       # –î–ª—è –∞—É–¥–∏—Ç–∞ –º–æ–∂–Ω–æ –∏ CPU, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –º–∏–ª–ª–∏–æ–Ω—ã\n",
        "    verbose= 100,\n",
        "    random_seed=CFG.RANDOM_STATE\n",
        ")\n",
        "\n",
        "audit_pool = Pool(\n",
        "    data=X_audit,\n",
        "    label=y_audit,\n",
        "    group_id=group_audit,\n",
        "    cat_features=audit_cat\n",
        ")\n",
        "\n",
        "audit_model.fit(audit_pool)\n",
        "\n",
        "# –í—ã–≤–æ–¥ –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
        "fi = audit_model.get_feature_importance(type='PredictionValuesChange')\n",
        "fi_df = pd.DataFrame({'feature': X_audit.columns, 'importance': fi}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 50 Features:\")\n",
        "print(fi_df.head(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuJyakzWDgU8"
      },
      "outputs": [],
      "source": [
        "# 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "train_dataset_v8 = generate_hybrid_train_data_v8(train_df, n_hard=CFG.TOP_K_CANDIDATES, n_random=CFG.NUM_RANDOM_NEGS, n_folds=CFG.N_FOLDS_ALS)\n",
        "\n",
        "# 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏—á–µ–π\n",
        "X_train_v8 = feature_engineering_v8(train_dataset_v8)\n",
        "\n",
        "print(\"\\n--- Training CatBoost Ranker (v8) ---\")\n",
        "# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –≥—Ä—É–ø–ø–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "X_train_v8.sort_values(by='user_id', inplace=True)\n",
        "\n",
        "# –°–ø–ª–∏—Ç\n",
        "train_users, val_users = train_test_split(\n",
        "    X_train_v8['user_id'].unique(),\n",
        "    test_size=CFG.VAL_SIZE,\n",
        "    random_state=CFG.RANDOM_STATE\n",
        ")\n",
        "\n",
        "train_mask = X_train_v8['user_id'].isin(train_users)\n",
        "val_mask = X_train_v8['user_id'].isin(val_users)\n",
        "\n",
        "drop_cols = ['user_id', 'book_id', 'target']\n",
        "feat_cols = [c for c in X_train_v8.columns if c not in drop_cols]\n",
        "# –ê–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π\n",
        "cat_feats = [c for c in CFG.CAT_COLS if c in feat_cols]\n",
        "\n",
        "print(f\"Features: {len(feat_cols)}\")\n",
        "\n",
        "train_pool = Pool(\n",
        "    data=X_train_v8.loc[train_mask, feat_cols],\n",
        "    label=X_train_v8.loc[train_mask, 'target'],\n",
        "    group_id=X_train_v8.loc[train_mask, 'user_id'],\n",
        "    cat_features=cat_feats\n",
        ")\n",
        "\n",
        "val_pool = Pool(\n",
        "    data=X_train_v8.loc[val_mask, feat_cols],\n",
        "    label=X_train_v8.loc[val_mask, 'target'],\n",
        "    group_id=X_train_v8.loc[val_mask, 'user_id'],\n",
        "    cat_features=cat_feats\n",
        ")\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã v8\n",
        "# YetiRank –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –≥—Ä–∞–¥–∞—Ü–∏—è–º–∏ 0, 1, 2\n",
        "params = {\n",
        "    'loss_function': 'YetiRank',\n",
        "    'iterations': 4000,\n",
        "    'learning_rate': 0.05,\n",
        "    'depth': 8,               # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É, —Ç.–∫. –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞–ª–æ –±–æ–ª—å—à–µ (Hard+Random)\n",
        "    'l2_leaf_reg': 7,\n",
        "    'random_seed': CFG.RANDOM_STATE,\n",
        "    'eval_metric': 'NDCG:top=20',\n",
        "    'early_stopping_rounds': 150,\n",
        "    'verbose': 100,\n",
        "    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU'\n",
        "}\n",
        "\n",
        "model_v8 = CatBoostRanker(**params)\n",
        "model_v8.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "print(f\"Best Score: {model_v8.get_best_score()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKkCnxC5Di5l"
      },
      "outputs": [],
      "source": [
        "print(\"--- [v8] Final Inference & Submission ---\")\n",
        "\n",
        "# 1. –ì–æ—Ç–æ–≤–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
        "candidates_exploded = []\n",
        "candidates_source = candidates_df.copy()\n",
        "\n",
        "for _, row in candidates_source.iterrows():\n",
        "    u_id = row['user_id']\n",
        "    if pd.isna(row['book_id_list']):\n",
        "        continue\n",
        "    b_ids = str(row['book_id_list']).split(',')\n",
        "    for b_id in b_ids:\n",
        "        if b_id:\n",
        "            candidates_exploded.append([u_id, int(b_id)])\n",
        "\n",
        "test_df = pd.DataFrame(candidates_exploded, columns=['user_id', 'book_id'])\n",
        "print(f\"Test pairs count: {len(test_df)}\")\n",
        "\n",
        "# 2. –°—á–∏—Ç–∞–µ–º ALS Score\n",
        "print(\"Retraining Global ALS for Test Scoring...\")\n",
        "all_rows = train_df['user_id'].map(user2idx).values\n",
        "all_cols = train_df['book_id'].map(item2idx).values\n",
        "sparse_master = sparse.csr_matrix((np.ones(len(all_rows)), (all_rows, all_cols)), shape=(num_users, num_items))\n",
        "\n",
        "master_model = implicit.als.AlternatingLeastSquares(\n",
        "    factors=CFG.ALS_FACTORS, regularization=CFG.ALS_REGULARIZATION,\n",
        "    iterations=CFG.ALS_ITERATIONS, random_state=CFG.RANDOM_STATE,\n",
        "    use_gpu=torch.cuda.is_available()\n",
        ")\n",
        "master_model.fit(sparse_master, show_progress=False)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º\n",
        "if hasattr(master_model.user_factors, \"to_numpy\"):\n",
        "    u_factors = master_model.user_factors.to_numpy()\n",
        "    i_factors = master_model.item_factors.to_numpy()\n",
        "else:\n",
        "    u_factors = master_model.user_factors\n",
        "    i_factors = master_model.item_factors\n",
        "\n",
        "test_u_idxs = test_df['user_id'].map(user2idx).fillna(-1).astype(int).values\n",
        "test_b_idxs = test_df['book_id'].map(item2idx).fillna(-1).astype(int).values\n",
        "\n",
        "test_scores = np.zeros(len(test_df))\n",
        "test_norms = np.zeros(len(test_df))\n",
        "mask = (test_u_idxs >= 0) & (test_b_idxs >= 0)\n",
        "\n",
        "if mask.sum() > 0:\n",
        "    test_scores[mask] = (u_factors[test_u_idxs[mask]] * i_factors[test_b_idxs[mask]]).sum(axis=1)\n",
        "    test_norms[mask] = np.linalg.norm(i_factors[test_b_idxs[mask]], axis=1)\n",
        "\n",
        "test_df['als_score'] = test_scores\n",
        "test_df['als_item_norm'] = test_norms\n",
        "\n",
        "# 3. –§–∏—á–∏\n",
        "# !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ !!!\n",
        "# –ú—ã –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º test_df, —á—Ç–æ–±—ã —É—á–µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ merge\n",
        "test_df = feature_engineering_v8(test_df)\n",
        "print(f\"Shape after Feature Engineering: {test_df.shape}\")\n",
        "\n",
        "# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫\n",
        "expected_cols = model_v8.feature_names_\n",
        "# –°–æ–∑–¥–∞–µ–º X_test –Ω–∞ –æ—Å–Ω–æ–≤–µ –û–ë–ù–û–í–õ–ï–ù–ù–û–ì–û test_df\n",
        "X_test = test_df.copy()\n",
        "\n",
        "for col in expected_cols:\n",
        "    if col not in X_test.columns:\n",
        "        X_test[col] = 0\n",
        "X_test = X_test[expected_cols]\n",
        "\n",
        "# 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "print(\"Predicting with CatBoost...\")\n",
        "final_scores = model_v8.predict(X_test)\n",
        "\n",
        "# –¢–µ–ø–µ—Ä—å –¥–ª–∏–Ω—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–≤–ø–∞–¥–∞—é—Ç\n",
        "test_df['score'] = final_scores\n",
        "\n",
        "# 6. –°–±–æ—Ä–∫–∞ —Å–∞–±–º–∏—Ç–∞\n",
        "test_df_sorted = test_df.sort_values(['user_id', 'score'], ascending=[True, False])\n",
        "# –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–∏ merge (–±–µ—Ä–µ–º —Å –≤—ã—Å—à–∏–º —Å–∫–æ—Ä–æ–º)\n",
        "test_df_sorted = test_df_sorted.drop_duplicates(subset=['user_id', 'book_id'])\n",
        "\n",
        "grouped_preds = test_df_sorted.groupby('user_id')['book_id'].apply(list).reset_index()\n",
        "\n",
        "final_submission = candidates_source[['user_id']].merge(grouped_preds, on='user_id', how='left')\n",
        "\n",
        "def format_list(x):\n",
        "    if isinstance(x, list):\n",
        "        # –¢–æ–ø 20\n",
        "        return \",\".join([str(i) for i in x[:20]])\n",
        "    return \"\"\n",
        "\n",
        "final_submission['book_id_list'] = final_submission['book_id'].apply(format_list)\n",
        "final_submission = final_submission[['user_id', 'book_id_list']]\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "save_name = 'submission_v8_only_hard.csv'\n",
        "final_submission.to_csv(save_name, index=False)\n",
        "print(f\"Submission saved to {save_name}\")\n",
        "print(final_submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMeDdWMS4xTH"
      },
      "outputs": [],
      "source": [
        "# --- –Ø–ß–ï–ô–ö–ê: –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ê–ù–ù–´–• –ù–ê GOOGLE DRIVE ---\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º Google –î–∏—Å–∫\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "# –ò–º—è –ø–∞–ø–∫–∏ –Ω–∞ –ì—É–≥–ª –î–∏—Å–∫–µ, –∫—É–¥–∞ –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è\n",
        "DRIVE_FOLDER_NAME = 'NTO_RecSys_Data'\n",
        "DESTINATION_PATH = f'/content/drive/MyDrive/{DRIVE_FOLDER_NAME}'\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å (–¥–æ–±–∞–≤—å —Å—é–¥–∞ names, –µ—Å–ª–∏ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ)\n",
        "FILES_TO_SAVE = [\n",
        "    'train.csv',\n",
        "    'candidates.csv',\n",
        "    'users.csv',\n",
        "    'books.csv',\n",
        "    'book_genres.csv',\n",
        "    'book_descriptions.csv',\n",
        "    # 'submission_strict_v6_fixed.csv' # —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "]\n",
        "\n",
        "# 3. –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –Ω–∞ –¥–∏—Å–∫–µ, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
        "if not os.path.exists(DESTINATION_PATH):\n",
        "    os.makedirs(DESTINATION_PATH)\n",
        "    print(f\"Created folder: {DESTINATION_PATH}\")\n",
        "else:\n",
        "    print(f\"Folder exists: {DESTINATION_PATH}\")\n",
        "\n",
        "# 4. –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã\n",
        "print(\"-\" * 30)\n",
        "for file_name in FILES_TO_SAVE:\n",
        "    if os.path.exists(file_name):\n",
        "        try:\n",
        "            shutil.copy2(file_name, f\"{DESTINATION_PATH}/{file_name}\")\n",
        "            print(f\"‚úÖ Saved: {file_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving {file_name}: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è File not found in Colab: {file_name}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Backup complete. Files are in 'MyDrive/{DRIVE_FOLDER_NAME}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdmLBhiO5Ec-"
      },
      "outputs": [],
      "source": [
        "# --- –Ø–ß–ï–ô–ö–ê: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –° GOOGLE DRIVE ---\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º Google –î–∏—Å–∫\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "# –¢–∞ –∂–µ –ø–∞–ø–∫–∞, —á—Ç–æ –∏ –≤ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–µ\n",
        "DRIVE_FOLDER_NAME = 'NTO_RecSys_Data'\n",
        "SOURCE_PATH = f'/content/drive/MyDrive/{DRIVE_FOLDER_NAME}'\n",
        "DESTINATION_PATH = '/content'  # –¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è Colab\n",
        "\n",
        "# 3. –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n",
        "print(\"-\" * 30)\n",
        "if os.path.exists(SOURCE_PATH):\n",
        "    files = os.listdir(SOURCE_PATH)\n",
        "    count = 0\n",
        "    for file_name in files:\n",
        "        # –ö–æ–ø–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã (–Ω–µ –ø–∞–ø–∫–∏) –∏ –∏—Å–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
        "        full_file_name = os.path.join(SOURCE_PATH, file_name)\n",
        "        if os.path.isfile(full_file_name) and not file_name.startswith('.'):\n",
        "            shutil.copy2(full_file_name, DESTINATION_PATH)\n",
        "            print(f\"üì• Loaded: {file_name}\")\n",
        "            count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        print(\"Folder is empty.\")\n",
        "    else:\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Successfully loaded {count} files from Drive.\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Folder '{SOURCE_PATH}' not found on Google Drive.\")\n",
        "    print(\"Please run the 'Save to Drive' cell first or create the folder manually.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
